{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting XGBoost using IRIS test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load all required libraries / packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"xgboost\") - if not installed earlier\n",
    "library(xgboost)\n",
    "library(caret)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1000)\n",
    "data(iris)\n",
    "species = iris$Species\n",
    "label = as.integer(iris$Species)-1\n",
    "iris$Species = NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preparation: Split the data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=nrow(iris)\n",
    "train.index = sample(n,floor(0.74*n))\n",
    "train.data = as.matrix(iris[train.index,])\n",
    "train.label = label[train.index]\n",
    "test.data = as.matrix(iris[-train.index,])\n",
    "test.label = label[-train.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Preparation: Create xgb Dmatrix objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.train.object = xgb.DMatrix(data = train.data,label=train.label)\n",
    "xgb.test.object = xgb.DMatrix(data = test.data,label=test.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Preparation: Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The multi:softprob objective tells the algorithm to calculate probabilities for every \n",
    "# possible outcome (in this case, a probability for each of the three flower species), for \n",
    "# every observation.\n",
    "\n",
    "num_class = length(levels(species))\n",
    "params = list(\n",
    "  booster = \"gbtree\",\n",
    "  eta = 0.001,\n",
    "  max_depth = 5,\n",
    "  gamma = 3,\n",
    "  subsample = 0.74,\n",
    "  colsample_bytree = 1,\n",
    "  objective = \"multi:softprob\",\n",
    "  eval_metric = \"mlogloss\",\n",
    "  num_class = num_class\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Development: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the XGBoost classifer\n",
    "xgb.fit=xgb.train(\n",
    "  params=params,\n",
    "  data=xgb.train.object,\n",
    "  nrounds=10000,\n",
    "  nthreads=1,\n",
    "  early_stopping_rounds=15,\n",
    "  watchlist=list(val1=xgb.train.object,val2=xgb.test.object),\n",
    "  verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "##### xgb.Booster\n",
       "raw: 3 Mb \n",
       "call:\n",
       "  xgb.train(params = params, data = xgb.train.object, nrounds = 10000, \n",
       "    watchlist = list(val1 = xgb.train.object, val2 = xgb.test.object), \n",
       "    verbose = 0, early_stopping_rounds = 15, nthreads = 1)\n",
       "params (as set within xgb.train):\n",
       "  booster = \"gbtree\", eta = \"0.001\", max_depth = \"5\", gamma = \"3\", subsample = \"0.74\", colsample_bytree = \"1\", objective = \"multi:softprob\", eval_metric = \"mlogloss\", num_class = \"3\", nthreads = \"1\", silent = \"1\"\n",
       "xgb.attributes:\n",
       "  best_iteration, best_msg, best_ntreelimit, best_score, niter\n",
       "callbacks:\n",
       "  cb.evaluation.log()\n",
       "  cb.early.stop(stopping_rounds = early_stopping_rounds, maximize = maximize, \n",
       "    verbose = verbose)\n",
       "# of features: 4 \n",
       "niter: 3017\n",
       "best_iteration : 3002 \n",
       "best_ntreelimit : 3002 \n",
       "best_score : 0.256246 \n",
       "nfeatures : 4 \n",
       "evaluation_log:\n",
       "    iter val1_mlogloss val2_mlogloss\n",
       "       1      1.097309      1.097386\n",
       "       2      1.095949      1.096094\n",
       "---                                 \n",
       "    3016      0.164426      0.256262\n",
       "    3017      0.164426      0.256265"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Review the final model and results\n",
    "xgb.fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model Development: Predict new outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can predict new outcomes given the testing data set that we set aside earlier. \n",
    "# We use the predict function to predict the likelihood of each observation in test.data \n",
    "# of being each flower species.\n",
    "\n",
    "# Predict outcomes with the test data\n",
    "xgb.pred = predict(xgb.fit,test.data,reshape=T)\n",
    "xgb.pred = as.data.frame(xgb.pred)\n",
    "colnames(xgb.pred) = levels(species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluation: Identify the class with highest probability for each prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the predictions and identify the label (class) with the highest probability. \n",
    "# This allows us to evaluate the true performance of the model by comparing the actual \n",
    "# labels with the predicted labels.\n",
    "# Use the predicted label with the highest probability\n",
    "\n",
    "# Please donâ€™t forget to re-convert your labels back to the names of the species \n",
    "# by adding 1 back to the integer values\n",
    "\n",
    "xgb.pred$prediction = apply(xgb.pred,1,function(x) colnames(xgb.pred)[which.max(x)])\n",
    "xgb.pred$label = levels(species)[test.label+1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Evaluation: Check Accuracy of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Final prediction Accuracy is  94.87%\"\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of the predictions. This compares the true labels from the test data \n",
    "# set with the predicted labels (with the highest probability), and it represents the percent \n",
    "# of flower species that were accuracy predicted using the XGBoost model. My results suggest \n",
    "# that XGBoost can consistently achieve an accuracy of at least 90%!\n",
    "\n",
    "result = sum(xgb.pred$prediction == xgb.pred$label)/nrow(xgb.pred)\n",
    "print(paste(\"Final prediction Accuracy is \",sprintf(\"%1.2f%%\",100*result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
